{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_firstdf = '/Users/Artem_Boltaev/Documents/EPAM Projects/6. RecSys_course/source_code/recsys_course_epam/data/raw/recsys_task0_dataset.parquet'\n",
    "\n",
    "df = pd.read_parquet(path_to_firstdf, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1188 entries, 1 to 6040\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   ground_truth  1188 non-null   object\n",
      " 1   prediction    1188 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 27.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1193, 3408, 2355, 1287, 2804, 594, 919, 595, ...</td>\n",
       "      <td>[2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1357, 3068, 1537, 2194, 648, 2268, 3468, 1210...</td>\n",
       "      <td>[2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3421, 1394, 104, 2735, 1210, 1079, 1615, 1291...</td>\n",
       "      <td>[2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3468, 2951, 1214, 1036, 260, 2028, 480, 1198,...</td>\n",
       "      <td>[2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[2987, 2333, 1175, 2337, 1535, 1392, 866, 2770...</td>\n",
       "      <td>[2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ground_truth  \\\n",
       "user_id                                                      \n",
       "1        [1193, 3408, 2355, 1287, 2804, 594, 919, 595, ...   \n",
       "2        [1357, 3068, 1537, 2194, 648, 2268, 3468, 1210...   \n",
       "3        [3421, 1394, 104, 2735, 1210, 1079, 1615, 1291...   \n",
       "4        [3468, 2951, 1214, 1036, 260, 2028, 480, 1198,...   \n",
       "5        [2987, 2333, 1175, 2337, 1535, 1392, 866, 2770...   \n",
       "\n",
       "                                                prediction  \n",
       "user_id                                                     \n",
       "1        [2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...  \n",
       "2        [2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...  \n",
       "3        [2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...  \n",
       "4        [2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...  \n",
       "5        [2858, 260, 1196, 1198, 593, 2028, 318, 527, 2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['ground_truth'].apply(pd.Series).to_numpy()\n",
    "y_pred = df['prediction'].apply(pd.Series).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.HitRate@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hit_rate_at_k_score(y_true, y_pred, k: int):\n",
    "    \"\"\"\n",
    "    Calculate Hit Rate at k.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples, n_items)\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like of shape (n_samples, n_items)\n",
    "        Estimated target values.\n",
    "\n",
    "    k : int, default=6\n",
    "        Number of recommendations to take into account.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        A non-negative floating point value (the best value is 1.0).\n",
    "    \"\"\"\n",
    "    \n",
    "    hits = [np.in1d(pred[:k], fact).any() for fact, pred in zip(y_true, y_pred)]\n",
    "    return np.mean(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected scores:\n",
    "\n",
    "HitRate@3: 0.418\n",
    "\n",
    "HitRate@5: 0.492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4175084175084175"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate_at_k_score(y_true, y_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49158249158249157"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate_at_k_score(y_true, y_pred, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision_at_k_score(y_true, y_pred, k):\n",
    "    \"\"\"\n",
    "    Calculate mean average precision at k (MAP@k).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples, n_items)\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like of shape (n_samples, n_items)\n",
    "        Estimated target values.\n",
    "\n",
    "    k : int, default=6\n",
    "        Maximum number of recommendations to take into account.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        A non-negative floating point value (the best value is 1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def _average_precision_at_k_score(fact, pred):\n",
    "        hits = np.in1d(pred[:k], fact)\n",
    "        precision_at_k_list = [hits[:i].sum() / i for i in range(1, len(hits) + 1) if hits[i - 1]]\n",
    "        if precision_at_k_list:\n",
    "            return np.mean(precision_at_k_list)\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    point_scores_list = [_average_precision_at_k_score(fact, pred) for fact, pred in zip(y_true, y_pred)]\n",
    "    return np.average(point_scores_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPECTED VALUES\n",
    "\n",
    "MAP@3: 0.325\n",
    "\n",
    "MAP@5: 0.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3247053872053872"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision_at_k_score(y_true, y_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3326155069210625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision_at_k_score(y_true, y_pred, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.NDCG@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG@k\n",
    "#https://gist.github.com/mblondel/7337391\n",
    "\n",
    "def dcg_score(y_true, y_pred, k, gains=\"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_pred : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG @k : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_pred, k, gains=\"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_pred : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k : float\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_pred, k, gains)\n",
    "    return actual / best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPECTED Values\n",
    "\n",
    "NDCG@3: 0.238\n",
    "\n",
    "NDCG@5: 0.223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/bfrczh690hggqb7bqhybgv040000gp/T/ipykernel_53670/907325949.py:24: RuntimeWarning: overflow encountered in power\n",
      "  gains = 2 ** y_true - 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(y_true[0], y_pred[0], 3, gains=\"exponential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/bfrczh690hggqb7bqhybgv040000gp/T/ipykernel_53670/907325949.py:24: RuntimeWarning: overflow encountered in power\n",
      "  gains = 2 ** y_true - 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(y_true[0], y_pred[0], 5, gains=\"exponential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.193e+03, 3.408e+03, 2.355e+03, 1.287e+03, 2.804e+03, 5.940e+02,\n",
       "       9.190e+02, 5.950e+02, 9.380e+02, 2.398e+03, 2.918e+03, 1.035e+03,\n",
       "       2.791e+03, 2.018e+03, 3.105e+03, 2.797e+03, 1.270e+03, 5.270e+02,\n",
       "       4.800e+01, 1.097e+03, 1.721e+03, 1.545e+03, 2.294e+03, 3.186e+03,\n",
       "       1.566e+03, 5.880e+02, 1.907e+03, 7.830e+02, 1.836e+03, 1.022e+03,\n",
       "       2.762e+03, 1.500e+02, 1.000e+00, 1.961e+03, 1.962e+03, 2.692e+03,\n",
       "       2.600e+02, 1.028e+03, 1.029e+03, 1.207e+03, 2.028e+03, 5.310e+02,\n",
       "       3.114e+03, 6.080e+02, 1.246e+03,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan,\n",
       "             nan,       nan,       nan,       nan,       nan,       nan])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f66966e931fc4ac787e20e3fb1743ecad89abfd01a532fa0096792975cac6b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
